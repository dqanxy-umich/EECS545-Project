{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Filter test\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "import umap\n",
        " \n",
        "def uniform(data):\t\t\t\t# UMAP dimension reduction and visualize\n",
        "  um=umap.UMAP(n_components=2,n_neighbors=15,min_dist=0.1)\n",
        "  data=um.fit_transform(data)\n",
        "  return data\n",
        " \n",
        "def principal(data):\t\t\t\t# PCA dimension reduction\n",
        "  pca=PCA()\n",
        "  data=pca.fit_transform(data) \n",
        "  detect=np.cumsum(pca.explained_variance_ratio_)\n",
        "  detect=np.where(detect>=0.9)  # extract the main component which can reflect 90% correctness\n",
        "  data=data[:,detect[0]]\n",
        "  return data\n",
        " \n",
        "def pre_process(data,label):\t\t\t# Introduce .mat dataset \n",
        "  data = data['data']\n",
        "  label = group['stim']\n",
        "  b,a = signal.butter(4, (20,20.5),'bandpass',fs=1000)\t# BETA bandpass filter\n",
        "  data = signal.filtfilt(b,a,data.T)\n",
        "  data = data.T\n",
        "  index,_=np.where(label<=0)\t\t\t    # data includes rest and experiment state\n",
        "  data = np.delete(data, index, 0)\t\t# we will delete those unwanted state\n",
        "  label = np.delete(label,index,0)\t\t# leave fingerflex we only want\n",
        "  label = label-1\n",
        "  label = label[:,0]\n",
        "  return data,label\n",
        " \n",
        "data = scipy.io.loadmat('drive/MyDrive/train/bp_data.mat')\n",
        "group = scipy.io.loadmat('drive/MyDrive/train/bp_stim.mat')\n",
        "bp,bplabel=pre_process(data,group)\n",
        " \n",
        "# dimension reduction \n",
        "bp=principal(bp)\n",
        "bp=uniform(bp)\n",
        "# visualize\n",
        "plt.figure(1,figsize=(15,15))\n",
        "scatter=plt.scatter(bp[:,0],bp[:,1],s=0.1,c=bplabel)\n",
        "plt.legend(handles=scatter.legend_elements()[0],labels=['thumb','index','middle','ring','little'])\n",
        "plt.title('After the bandpass filter')\n"
      ],
      "metadata": {
        "id": "AIpblberatTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the Waveforms from different patients\n",
        "def data_process(data,group):\n",
        "  data=data['data']\n",
        "  group=group['stim']\n",
        "  b,a = signal.butter(4, (20.1,20.5),'bandpass',fs=1000)\n",
        "  data = signal.filtfilt(b,a,data.T)\n",
        "  data = data.T\n",
        "  index,_=np.where(group==1) # Only plot thumb for this case\n",
        "  train = data[index,:]\n",
        "  trainlabel=group[index,:]\n",
        "  train=principal(train)\n",
        "  return train,trainlabel\n",
        "\n",
        "# Load data and Preprocess \n",
        "data = scipy.io.loadmat('drive/MyDrive/train/bp_data.mat')\t\n",
        "group = scipy.io.loadmat('drive/MyDrive/train/bp_stim.mat')\n",
        "bp,_=data_process(data,group)\n",
        "data = scipy.io.loadmat('drive/MyDrive/train/ht_data.mat')\n",
        "group = scipy.io.loadmat('drive/MyDrive/train/ht_stim.mat')\n",
        "ht,_=data_process(data,group)\n",
        "# plot\n",
        "timestep=np.arange(15000)\t\t\n",
        "plt.figure(1,figsize=(15,3))\n",
        "plt.plot(timestep,bp[timestep,0])\n",
        "plt.xlabel('timestep')\n",
        "plt.ylabel('voltage')\n",
        "plt.title('Patient A thumb waveform')\n",
        "plt.legend(['Principal Channel one'])\n",
        "plt.figure(2,figsize=(15,3))\n",
        "plt.plot(timestep,ht[timestep,0])\n",
        "plt.xlabel('timestep')\n",
        "plt.ylabel('voltage')\n",
        "plt.title('Patient B thumb waveform')\n",
        "plt.legend(['Principal Channel one'])"
      ],
      "metadata": {
        "id": "69VC1PpNbcpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN LSTM UMAP-Linear Classifier\n",
        "import scipy.io\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from torch.utils.data import DataLoader, sampler, TensorDataset \n",
        " \n",
        "def uniform(data):\t\t\t\t# UMAP dimension reduction\n",
        "  um=umap.UMAP(n_components=30,n_neighbors=15,min_dist=0.1)\n",
        "  data=um.fit_transform(data)\n",
        "  return data\n",
        " \n",
        "def principal(data):\t\t\t\t# PCA dimension reduction\n",
        "  pca=PCA(n_components=30)  # Using main 30 channels \n",
        "  data=pca.fit_transform(data) \n",
        "  return data\n",
        "\n",
        "def data_process(data,group):\n",
        "  data=data['data']\n",
        "  group=group['stim']\n",
        "  b,a = signal.butter(4, (20.1,20.5),'bandpass',fs=1000)\n",
        "  data = signal.filtfilt(b,a,data.T)\n",
        "  data = data.T\n",
        "  index,_=np.where(group>0)\n",
        "  train = data[index,:]\n",
        "  trainlabel=group[index,0]\n",
        "  # UMAP-Linear model \n",
        "  # train=uniform(train)\n",
        "  train=principal(train)\n",
        "  trainlabel=trainlabel-1  # Five class for 1,2,3,4,5 but pytorch classification require 0-4\n",
        "  train,trainlabel=torch.from_numpy(train).float(),torch.from_numpy(trainlabel).long()\n",
        "  return train,trainlabel\n",
        " \n",
        "class myNetwork(nn.Module):\n",
        "    def __init__(self, channel ,hidden_size, num_states):\n",
        "        super().__init__()\n",
        "        # CNN \n",
        "        '''\n",
        "        self.conv1= nn.Conv1d(channel,32,20,stride=1)\n",
        "        self.conv2= nn.Conv1d(32,64,10)\n",
        "        self.conv3= nn.Conv1d(64,128,8)\n",
        "        self.flat= nn.Flatten()\n",
        "        '''\n",
        "        # LSTM\n",
        "        '''\n",
        "        self.rnn=nn.LSTM(input_size=channel,\n",
        "                         hidden_size=hidden_size,\n",
        "                         num_layers=1,\n",
        "                         batch_first=True,\n",
        "                         )\n",
        "        '''\n",
        "        # linear layer\n",
        "        self.bn1 = nn.BatchNorm1d(channel)\n",
        "        self.fc1 = nn.Linear(channel,hidden_size)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size,hidden_size)\n",
        "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size,hidden_size)\n",
        "        self.bn4 = nn.BatchNorm1d(hidden_size)\n",
        "        self.fc4 = nn.Linear(hidden_size,num_states)\n",
        " \n",
        "    def forward(self, x):\n",
        "        # CNN\n",
        "        '''\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.flat(x)\n",
        "        '''\n",
        "        # RNN\n",
        "        '''\n",
        "        rout,(h_n,h_c)=self.rnn(x,None)\n",
        "        x = self.bn2(rout[:,-1,:])\n",
        "        '''\n",
        "        x = self.bn1(x)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.bn4(x)\n",
        "        scores = self.fc4(x)\n",
        "        return scores\n",
        " \n",
        "def init_normal(m): #initial linear layer parameters with normal distribution\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.kaiming_normal_(m.weight,nonlinearity='relu')\n",
        " \n",
        "# Load data and Preprocess \n",
        "data = scipy.io.loadmat('drive/MyDrive/train/bp_data.mat')\n",
        "group = scipy.io.loadmat('drive/MyDrive/train/bp_stim.mat')\n",
        "bp,bplabel=data_process(data,group)\n",
        "data = scipy.io.loadmat('drive/MyDrive/train/ht_data.mat')\n",
        "group = scipy.io.loadmat('drive/MyDrive/train/ht_stim.mat')\n",
        "ht,htlabel=data_process(data,group)\n",
        "data = scipy.io.loadmat('drive/MyDrive/train/jp_data.mat')\n",
        "group = scipy.io.loadmat('drive/MyDrive/train/jp_stim.mat')\n",
        "jp,jplabel=data_process(data,group)\n",
        "data = scipy.io.loadmat('drive/MyDrive/train/wc_data.mat')\n",
        "group = scipy.io.loadmat('drive/MyDrive/train/wc_stim.mat')\n",
        "wc,wclabel=data_process(data,group)\n",
        "data = scipy.io.loadmat('drive/MyDrive/train/zt_data.mat')\n",
        "group = scipy.io.loadmat('drive/MyDrive/train/zt_stim.mat')\n",
        "zt,ztlabel=data_process(data,group)\n",
        "data = scipy.io.loadmat('drive/MyDrive/train/jc_data.mat')\n",
        "group = scipy.io.loadmat('drive/MyDrive/train/jc_stim.mat')\n",
        "jc,jclabel=data_process(data,group)\n",
        "data = scipy.io.loadmat('drive/MyDrive/test/mv_data.mat')\n",
        "group = scipy.io.loadmat('drive/MyDrive/test/mv_stim.mat')\n",
        "test,testlabel=data_process(data,group)\n",
        "\n",
        "# Stack all nerual data\n",
        "train=torch.vstack((bp,ht,jp,wc,zt,jc)) \n",
        "trainlabel=torch.hstack((bplabel,htlabel,jplabel,wclabel,ztlabel,jclabel))\n",
        "channel=30\n",
        "\n",
        "# LSTM random path assignment\n",
        "'''\n",
        "path = 5\n",
        "for recurrent in range(path):\n",
        "  pick=np.random.choice(channel,channel,replace='False') \n",
        "  train=torch.hstack((train,train[:,pick]))\n",
        "  test=torch.hstack((test,test[:,pick]))\n",
        "train=train.view(train.shape[0],path+1,-1)\n",
        "test=test.view(test.shape[0],path+1,-1)\n",
        "'''\n",
        "# CNN reshape \n",
        "'''\n",
        "train=train.view(train.shape[0],channel,-1)\n",
        "test=test.view(test.shape[0],channel,-1)\n",
        "# Previous data\n",
        "column=train\n",
        "col=test\n",
        "for i in range(19):   # 20ms window so we need past 19ms data as extra input\n",
        "  column=torch.roll(column,1, 0)\n",
        "  column[0,:,0]=0\n",
        "  col=torch.roll(col,1, 0)\n",
        "  col[0,:,0]=0\n",
        "  train=torch.dstack((train,column))\n",
        "  test=torch.dstack((test,col))\n",
        "'''\n",
        " \n",
        "# Create Dataset and dataloader\n",
        "test_ds= TensorDataset(test, testlabel)\n",
        "train_ds = TensorDataset(train, trainlabel)\n",
        "train_dl = DataLoader(train_ds, batch_size=400000, shuffle=True, drop_last=True)\n",
        "test_dl =  DataLoader(test_ds, batch_size=40000, shuffle=False, drop_last=True)\n",
        " \n",
        "# parameter & loss function & optimize\n",
        "learning_rate = 0.01\n",
        "decay=0.001\n",
        "hidden=400\n",
        "outlayer=5\n",
        " \n",
        "theNetwork=myNetwork(channel,hidden,outlayer)\n",
        "theNetwork.apply(init_normal)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(theNetwork.parameters(),lr=learning_rate,weight_decay=decay)\n",
        "epoch=1 # we will stop whenever we want\n",
        "# start training\n",
        "while epoch!=0:\n",
        "  for x,y in train_dl: # batch of training points\n",
        "    optimizer.zero_grad()\n",
        "    predict=theNetwork(x)\n",
        "    loss=criterion(predict,y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  # valid per epoch\n",
        "  with torch.no_grad():\n",
        "    accu=0\n",
        "    length=0\n",
        "    for x,y in train_dl: #current train error\n",
        "      output=theNetwork(x)\n",
        "      _,predict=torch.max(output.data,1)\n",
        "      accu=accu+(predict==y).sum()\n",
        "      length=length+len(y)\n",
        "    traincorrect=accu/length\n",
        "    accu=0\n",
        "    length=0\n",
        "    for xval,yval in test_dl:# current validation error\n",
        "      output=theNetwork(xval)\n",
        "      _,predict=torch.max(output.data,1)\n",
        "      accu=accu+(predict==yval).sum()\n",
        "      length=length+len(yval)\n",
        "    testcorrect=accu/length\n",
        "    print(epoch,traincorrect,testcorrect)\n",
        "    epoch=epoch+1"
      ],
      "metadata": {
        "id": "Jgxy8PXqcP83"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "ECoG classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}